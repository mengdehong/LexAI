好的，收到。作为本项目的技术负责人，我将基于我们已经确定的`V3.1 最终架构规约`，为你制定一份极其详尽、可执行、且高度"AI-Coding-Friendly"的开发路线图。

这份路线图不仅是任务清单，更是一份战术手册。每一阶段都明确了其核心哲学、关键依赖（附带文档链接，便于快速查阅和作为AI Coding的上下文）、具体到代码层面的任务拆解，以及可量化的验收标准。这种结构化设计旨在最大化开发效率，尤其是当与AI编程助手（如GitHub Copilot）协同工作时。

---

## LexAI：项目开发路线图 (V1.0)

### 总体原则

1.  **风险前置 (De-risk First):** 最复杂、最具不确定性的技术点（如Rust/Python混合编程）必须在项目初期就被攻克和验证。
2.  **数据主干先行 (Data Spine First):** 优先构建从数据输入（文档上传）到数据处理（向量化）再到数据检索（RAG）的核心后端管道。
3.  **价值闭环驱动 (Value Loop Driven):** 尽快打通一个最小但完整的用户价值闭环（上传->提取->学习），即便初期功能简陋。
4.  **配置化延迟 (Configuration Last):** 将高度灵活的配置功能（如模型映射）放在核心流程稳定后再实现，避免过早优化。

---

### Phase 0: 项目启动与基础设施 (1周)

*   **核心原则:** **Fail Fast, Verify Early (快速失败，尽早验证)**。此阶段的唯一目标是扫清所有环境和技术栈集成障碍，确保客户端与后端的骨干网络、后端内部的混合编程模型能够顺利“握手”。

*   **核心依赖:**
    *   **Tauri:** [v2 Docs](https://v2.tauri.app/) | [GitHub](https://github.com/tauri-apps/tauri)
    *   **Rust:** [The Book](https://doc.rust-lang.org/book/)
    *   **PyO3:** [User Guide](https://pyo3.rs/v0.21.2/) | [GitHub](https://github.com/PyO3/pyo3)
    *   **Maturin:** [User Guide](https://www.maturin.rs/user_guide) (用于构建和发布PyO3项目)
    *   **FastAPI:** [Docs](https://fastapi.tiangolo.com/) | [GitHub](https://github.com/tiangolo/fastapi)
    *   **sqlx:** [Docs](https://docs.rs/sqlx/latest/sqlx/) | [GitHub](https://github.com/launchbadge/sqlx)
    *   **Poetry:** [Docs](https://python-poetry.org/docs/) (用于Python依赖管理)

*   **具体任务 (AI-Coding Friendly):**
    1.  **仓库初始化:**
        *   `[CMD]` 在GitHub上创建名为 `lex-ai` 的Monorepo。
        *   `[CMD]` 在根目录创建 `backend` 和 `client` 两个文件夹。
        *   `[PROMPT]` "为这个Monorepo项目创建一个顶层README.md，简要说明项目结构和目标。"
    2.  **后端骨架 (Backend):**
        *   `[CMD]` 进入 `backend` 目录, 使用 `poetry init` 初始化Python项目。添加 `fastapi`, `uvicorn`, `qdrant-client`。
        *   `[PROMPT]` "创建一个 `backend/app/main.py` 文件，包含一个最简单的FastAPI应用，带有一个根路由 `/` 返回 `{'status': 'ok'}`。"
        *   `[PROMPT]` "**【关键任务】** 在 `backend` 目录下创建一个名为 `rust_core` 的新Rust库 (`cargo new --lib rust_core`)。配置其 `Cargo.toml` 以使用 PyO3。在 `lib.rs` 中，创建一个名为 `hello_from_rust` 的 `#[pyfunction]`，它返回一个字符串 'Hello from Rust!'。配置 `pyproject.toml` 以便Maturin能构建此库。"
        *   `[CMD]` 使用 `maturin develop` 构建Rust模块。
        *   `[PROMPT]` "修改 `backend/app/main.py`，导入 `rust_core` 模块，并添加一个新路由 `/test_rust`，调用 `hello_from_rust()` 函数并返回其结果。"
    3.  **客户端骨架 (Client):**
        *   `[CMD]` 进入 `client` 目录, 使用 `cargo create-tauri-app` (选择你熟悉的前端框架，如React/Vue/Svelte)。
        *   `[PROMPT]` "在 `client/src-tauri/src/main.rs` 中，集成 `sqlx`。编写一个异步 `main` 函数，并添加一个简单的 `run_migrations` 函数，用于连接到一个本地SQLite文件 (`lexai.db`) 并创建一张 `terms` 表 (包含 id, term, definition 字段)。"
        *   `[PROMPT]` "在Tauri的 `main.rs` 中，创建一个 `#[tauri::command]` 异步函数 `fetch_backend_status`，它使用 `reqwest` 库向 `http://127.0.0.1:8000/` 发送GET请求，并返回响应文本。"
        *   `[PROMPT]` "在前端代码中（例如 App.tsx），使用 `invoke` 调用 `fetch_backend_status` 命令，并将结果显示在页面上。"

*   **验收标准:**
    *   [ ] `backend` 目录下的 `poetry run uvicorn app.main:app` 能够成功启动。
    *   [ ] 访问 `http://127.0.0.1:8000/test_rust` 能看到 `"Hello from Rust!"`。
    *   [ ] `client` 目录下的 `npm run tauri dev` 能够成功启动桌面应用。
    *   [ ] 桌面应用界面上能成功显示从后端获取的 `{"status":"ok"}`。
    *   [ ] 客户端启动时，会在 `src-tauri` 目录下生成一个 `lexai.db` 文件。

---

### Phase 1: 后端核心管道贯通 (2周)

*   **核心原则:** **Build the Spine First (优先构建主干)**。集中精力打通文档处理的完整数据流，从API接收文件到最终向量化存储，这是整个应用能力的基础。

*   **核心依赖:**
    *   **Rust解析库:** [extractous](https://github.com/foss-dev/extractous) (一个优秀的、支持多种格式的文本提取库)
    *   **文本分割:** [text-splitter](https://docs.rs/text-splitter/latest/text_splitter/) (Rust原生) 或 LangChain (Python)
    *   **向量化:** [sentence-transformers](https://www.sbert.net/) (通过Python调用)
    *   **向量数据库:** [Qdrant Python Client](https://qdrant.tech/documentation/clients/python/)

*   **具体任务 (AI-Coding Friendly):**
    1.  **Rust 性能核心:**
        *   `[PROMPT]` "扩展 `rust_core` 模块。添加 `extractous` 依赖。创建一个新的 `#[pyfunction]` 名为 `extract_text`，它接受一个文件路径作为输入，返回提取出的纯文本内容。需要处理可能的I/O和解析错误。"
    2.  **FastAPI 文档处理管道:**
        *   `[PROMPT]` "在FastAPI中，创建一个新的POST路由 `/documents/upload`。使用 `UploadFile` 类型接收上传的文档。将文件临时保存到磁盘。"
        *   `[PROMPT]` "在 `/documents/upload` 路由的逻辑中，调用 `rust_core.extract_text` 函数处理临时文件，获取纯文本。"
        *   `[PROMPT]` "实现一个文本分割逻辑。输入是长文本，输出是一个字符串数组，每个字符串是一个文本块（chunk）。可以先用简单的按段落分割，或引入LangChain的`RecursiveCharacterTextSplitter`。"
        *   `[PROMPT]` "初始化 `sentence-transformers` 模型。遍历所有文本块，将它们批量编码成向量。"
        *   `[PROMPT]` "初始化 `qdrant-client`。创建一个名为 `documents` 的集合。将文本块、对应的向量以及元数据（如文档ID、块索引）批量存入Qdrant。"
    3.  **FastAPI 例句检索API:**
        *   `[PROMPT]` "创建一个GET路由 `/documents/{doc_id}/search`，接受一个查询参数 `term`。"
        *   `[PROMPT]` "在该路由中，将查询 `term` 向量化，然后调用Qdrant的 `search` API，使用元数据过滤器确保只在指定的 `doc_id` 中搜索。返回得分最高的3-5个文本块。"
    4.  **健壮性与测试:**
        *   `[PROMPT]` "使用 `pytest` 为 `/documents/upload` 和 `/documents/{doc_id}/search` 编写集成测试。需要准备一个样本PDF文件，并模拟API调用。"

*   **验收标准:**
    *   [ ] 通过 `curl` 或 Postman 上传一个PDF文件到 `/documents/upload`，后端不报错，并能观察到Qdrant中增加了新的向量点。
    *   [ ] 调用 `/documents/{doc_id}/search?term=...` 接口，能够返回包含该术语的、结构正确的JSON数据。
    *   [ ] `pytest` 中的所有测试用例都能通过。

---

### Phase 2: 客户端核心工作流闭环 (3周)

*   **核心原则:** **User Value First (用户价值优先)**。目标是让用户能完成一次完整的核心体验：上传文档 -> 查看提取结果 -> 点击术语查看例句 -> AI释义。暂时忽略配置的复杂性。

*   **核心依赖:**
    *   **前端UI库:** (你选择的框架，如React/Vue/Svelte)
    *   **前端状态管理:** [Zustand](https://github.com/pmndrs/zustand) (React) 或 [Pinia](https://pinia.vuejs.org/) (Vue)
    *   **Tauri API:** [JS/TS API Docs](https://v2.tauri.app/reference/js/)

*   **具体任务 (AI-Coding Friendly):**
    1.  **实现模块三 UI & 逻辑:**
        *   `[PROMPT]` "创建一个 `DocumentList` 组件，显示已上传的文档列表。创建一个 `Uploader` 组件，允许用户选择本地文件并通过Tauri的 `http` 模块或直接的JS `fetch` 上传到后端。"
        *   `[PROMPT]` "创建一个 `DocumentView` 页面。当用户点击文档时，该页面展示文档内容。添加一个 'Extract Terms' 按钮。"
        *   `[PROMPT]` "为 'Extract Terms' 按钮实现点击逻辑：调用后端LLM服务（**暂时硬编码一个OpenAI API调用**），将当前页面文本发送过去，并解析返回的JSON术语列表。"
        *   `[PROMPT]` "将返回的术语列表显示在 `TermSidebar` 组件中。为每个术语实现点击事件，调用后端的 `/search` API来获取上下文例句，并将结果显示在主视图中。"
        *   `[PROMPT]` "当用户选中一个例句时，显示一个 'AI Explain' 按钮。点击后，再次调用LLM服务，对该句子进行释义。"
    2.  **实现模块一 本地存储:**
        *   `[PROMPT]` "在 `src-tauri/src/db.rs` 中，使用 `sqlx` 实现 `add_term(term, definition)`、`get_all_terms()`、`delete_term(id)` 等函数。"
        *   `[PROMPT]` "将这些数据库函数封装成 `#[tauri::command]`，以便前端调用。"
        *   `[PROMPT]` "在 `TermSidebar` 组件中，为每个术语添加一个 'Add to Global' 按钮。点击后，调用Tauri命令将术语存入本地SQLite数据库。"
        *   `[PROMPT]` "创建一个 `TermbaseView` 页面，用于显示和管理全局术语库中的所有术语。"

*   **验收标准:**
    *   [ ] 用户可以成功上传一个PDF文档。
    *   [ ] 在文档视图中，点击“提取术语”能成功从（硬编码的）LLM获取并显示术语列表。
    *   [ ] 点击列表中的任意术语，能从Qdrant中检索并展示3-5个相关的原文例句。
    *   [ ] 选中例句并点击“AI解释”，能看到LLM返回的释义。
    *   [ ] 用户可以把术语添加到全局术语库，并在一个单独的页面看到所有已添加的术语。

---

### Phase 3: 高级功能与配置 (2周)

*   **核心原则:** **Empower the Expert (赋能专家用户)**。现在核心流程已稳定，开始构建我们应用的核心差异化特性——极致的可配置性。

*   **核心依赖:**
    *   **安全凭证存储:** [keyring-rs](https://docs.rs/keyring/latest/keyring/) (通过Tauri Rust核心)
    *   **客户端状态持久化:** [tauri-plugin-store](https://github.com/tauri-apps/plugins-workspace/tree/v2/plugins/store)

*   **具体任务 (AI-Coding Friendly):**
    1.  **实现模块二 (AI服务配置中心):**
        *   `[PROMPT]` "创建一个 `Settings` 页面。在其中设计一个UI，允许用户输入服务商名称、API密钥和可选的Base URL。使用 `keyring-rs` 创建Tauri命令 `save_api_key` 和 `get_api_key`，确保密钥被安全地存储在系统钥匙串中，而不是明文存储。"
        *   `[PROMPT]` "在 `Settings` 页面，创建一个表格，行是功能（如'对话式引导'），列是模型选择下拉框。下拉框的数据源是用户已配置的服务商。"
        *   `[PROMPT]` "使用 `tauri-plugin-store` 将用户的模型映射配置持久化到本地。例如，保存为一个JSON对象 `{'term_extraction': 'openai/gpt-4o', 'explanation': 'anthropic/claude-3-sonnet'}`。"
    2.  **重构API调用逻辑:**
        *   `[PROMPT]` "**【核心重构】** 创建一个统一的API调用模块（例如 `src/lib/llm_dispatcher.ts`）。这个模块在执行任何LLM调用前，会先从 `tauri-plugin-store` 读取用户的模型映射配置，根据功能名称找到对应的模型，然后调用Tauri命令获取该模型服务商的API密钥，最后才构建和发送请求。"
        *   `[CMD]` 将Phase 2中所有硬编码的LLM调用全部替换为通过 `llm_dispatcher` 调用。
    3.  **实现模块一 (对话式引导):**
        *   `[PROMPT]` "设计一个对话式UI，在用户首次启动应用时触发。根据对话流程，收集用户的领域、水平和目标。"
        *   `[PROMPT]` "将收集到的信息组合成一个高质量的Prompt，通过 `llm_dispatcher` 调用为'对话式引导'功能指定的模型。"
        *   `[PROMPT]` "解析LLM返回的术语JSON，并批量调用Tauri命令将其存入本地SQLite数据库，作为初始知识库。"
    4.  **实现闪卡与导出:**
        *   `[PROMPT]` "在 `TermbaseView` 中添加一个“导出为CSV”的功能，将数据库中的术语导出。"

*   **验收标准:**
    *   [ ] 用户可以在设置中添加、编辑、删除多个AI服务商的凭证。API密钥通过`keyring`安全存储。
    *   [ ] 用户可以将“文档术语提取”功能指派给GPT-4o，将“AI辅助释义”指派给Claude-3-Sonnet。
    *   [ ] 实际操作时，应用会严格按照用户的配置调用对应的模型。
    *   [ ] 新用户首次打开应用会进入对话式引导，并成功生成初始术语库。
    *   [ ] 全局术语库可以成功导出为CSV文件。

---

### Phase 4: 打磨、发布与文档化 (2周)

*   **核心原则:** **From Project to Product (从项目到产品)**。此阶段关注稳定性、用户体验、开发者体验和社区参与度。

*   **核心依赖:**
    *   **CI/CD:** [GitHub Actions](https://github.com/features/actions)
    *   **Tauri Action for CI/CD:** [tauri-action](https://github.com/tauri-apps/tauri-action)

*   **具体任务 (AI-Coding Friendly):**
    1.  **UI/UX 优化:**
        *   `[CMD]` 全面审查应用，添加加载状态（spinners）、错误提示（toasts）、空状态（empty states）。
        *   `[CMD]` 确保窗口大小调整、不同屏幕分辨率下的UI表现良好。
    2.  **CI/CD 自动化:**
        *   `[PROMPT]` "**【工程亮点】** 创建一个 `.github/workflows/release.yml` 文件。配置一个在创建`v*.*.*`格式的tag时触发的GitHub Action。"
        *   `[PROMPT]` "在该Action中，配置`tauri-action`，使其能够为Windows (msi), macOS (dmg) 和 Linux (AppImage, deb) 自动构建、签名和打包应用。"
        *   `[PROMPT]` "配置Action的最后一步，将构建产物自动上传到GitHub Release。"
    3.  **项目文档化:**
        *   `[PROMPT]` "撰写一份高质量的根目录 `README.md`。内容必须包括：项目愿景、核心功能截图、详细的架构图和技术栈说明、本地开发环境搭建指南 (`git clone` 后需要运行哪些命令)、贡献指南。"
    4.  **正式发布:**
        *   `[CMD]` 完成所有测试，合并所有功能到 `main` 分支。
        *   `[CMD]` 在Git中打上 `v1.0.0` 标签并推送到GitHub，触发自动化发布流程。
        *   `[CMD]` 验证发布流程是否成功，并从GitHub Release页面下载所有平台的安装包进行最终测试。

*   **验收标准:**
    *   [ ] 在GitHub上推送一个 `v1.0.0` 标签后，CI/CD流水线自动执行并通过。
    *   [ ] 在GitHub Release页面，可以找到为Windows, macOS, Linux生成的可直接安装的软件包。
    *   [ ] 一个不了解此项目的新开发者，可以根据 `README.md` 的指引，在30分钟内成功在本地将项目运行起来。
    *   [ ] v1.0.0 版本功能稳定，无明显崩溃或数据丢失问题。