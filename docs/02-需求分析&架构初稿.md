# 专业英语学习软件：V3.1 最终架构规约

## 一、 详细功能规约 (The What)

本节定义了软件的核心用户功能。后续所有的架构设计、技术选型和开发策略都旨在高效、稳定地实现这些需求。

### 模块一：对话式引导与全局术语库 (Onboarding & Global Termbase)

#### 1. 对话式引导 (Conversational Setup)
*   **描述:** 用户首次打开应用时，会进入一个由AI驱动的对话界面。该对话旨在收集用户的核心学习需求，并自动生成一个初始知识库。
*   **对话流程:**
    1.  **欢迎语:** "欢迎！我是您的AI语言学习助手。为了给您最好的体验，我需要了解一下您。您目前主要关注哪个专业领域？（例如：AI、密码学、流体力学）"
    2.  **获取领域:** 用户输入领域。
    3.  **评估水平:** "好的，关于`[用户领域]`，您的英语水平大概是？（例如：刚入门，能读懂论文，能熟练写作）"
    4.  **确认目标:** "您的主要目标是什么？（例如：快速掌握核心术语，提升论文阅读能力，备考专业英语考试）"
    5.  **生成术语库:** "感谢您的信息！根据您的需求，我将为您生成一个包含约200个`[用户领域]`核心术语的初始知识库。这可能需要一点时间..."
*   **实现:** 客户端将收集到的信息（领域、水平、目标）组合成一个精炼的Prompt，调用用户在下一步配置的"对话式引导"模型，生成全局术语库。
*   **工程价值:**
    *   **智能入场:** 展示了通过LLM提升用户初次体验（Onboarding）的能力。
    *   **动态Prompt工程:** 体现了根据用户输入动态构建高效Prompt的技巧。
    *   **端到端流程:** 涵盖了从用户输入、LLM调用到本地数据持久化的完整闭环。

#### 2. 全局术语库 (Global Termbase)
*   **描述:** 一个集中管理所有专业术语的个人知识库。
*   **来源:** 1) 对话式引导时AI生成；2) 用户从各文档提取后手动添加。
*   **功能:** 搜索、编辑、删除术语；与文档内提取的术语进行去重合并。
*   **工程价值:** 考验本地数据库（SQLite）的设计与交互能力，以及数据同步与一致性处理。

### 模块二：AI 服务配置中心 (Configuration Hub)

#### 1. 服务商管理 (Provider Management)
*   **描述:** 在设置界面，用户可以添加、编辑和删除多个AI服务商的凭证。
*   **UI设计:** 一个列表，每一项包含服务商名称 (如 "OpenAI", "Anthropic")、API密钥 (使用`keyring`安全存储，显示为 `sk-xxxx...xxxx`)，以及可选的 API Base URL (用于支持代理或私有部署模型)。
*   **工程价值:**
    *   **安全性:** 强调了安全处理敏感凭证（API密钥）的实践。
    *   **灵活性:** 支持多服务商和私有化部署，展示了架构的开放性。

#### 2. 功能-模型映射 (Function-to-Model Mapping)
*   **描述:** 用户可以将特定的功能精确地指派给已配置好的某个具体模型。这是本应用**“面向专家”**定位的核心体现。
*   **UI设计:** 一个设置表格，至少包含以下映射关系：
| 功能              | 指派的模型                                                   |
| :---------------- | :----------------------------------------------------------- |
| 对话式引导        | `[下拉框选择: OpenAI/gpt-4o, Anthropic/claude-3-sonnet, ...]` |
| 文档术语提取      | `[下拉框选择]`                                               |
| AI辅助释义        | `[下拉框选择]`                                               |
| (可选) AI语法分析 | `[下拉框选择]`                                               |
*   **实现:** 用户的选择被保存在本地配置文件中。当执行某个功能时，应用会读取该配置，选择对应的API密钥和模型名称来构建请求。
*   **工程价值:**
    *   **成本与性能优化:** 允许用户为简单任务（如释义）选择廉价快速的模型，为复杂任务（如术语提取）选择强大昂贵的模型，体现了对LLM应用成本效益的深刻理解。
    *   **高可配置性:** 系统核心逻辑与具体的LLM模型解耦，展示了高度模块化和可扩展的软件设计思想。

### 模块三：核心学习工作台 (Core Workbench)

#### 1. 文档处理与管理
*   **功能:** 支持用户上传本地文档（PDF, DOCX等），实时跟踪后端处理状态（解析、分块、向量化），并可对已处理的文档进行管理和删除。

#### 2. 一键术语提取
*   **描述:** 在文档阅读界面，一键调用LLM，提取当前页面或选定文本中的核心术语。
*   **Prompt策略:** 提供一个高质量的默认Prompt，并将其暴露在高级设置中供用户修改。
    ```prompt
    You are an expert in [Domain]. Analyze the following text from a document and identify the top 20 most critical technical terms. For each term, provide a concise, one-sentence definition suitable for a professional in the field.
    Return the result ONLY in a valid JSON array format: [{ "term": "TermName", "definition": "Concise definition."}].
    DO NOT include any other text or explanations in your response.
    ---
    Text:
    {document_chunk}
    ```
*   **工程价值:**
    *   **结构化输出:** 考验对LLM返回的结构化数据（JSON）的稳定解析和错误处理能力。
    *   **Prompt工程:** 展示了如何设计能产生稳定、可机读结果的Prompt。

#### 3. 上下文例句检索
*   **描述:** 点击任意一个提取出的术语，系统会立即从当前文档的向量化数据中，检索出包含该术语的3-5个最相关的原文例句。
*   **实现:** 这是一个纯粹的后端RAG检索任务，对Qdrant的低延迟查询能力有较高要求。

#### 4. AI辅助释义与用法
*   **描述:** 用户可以选中任意例句，触发一个新请求，调用为“AI辅助释义”功能指定的LLM，对该句子进行更详细的解释、语法分析或同义句改写。

#### 5. 简易闪卡系统 & 导出
*   **描述:** 用户可以将重要的术语和例句添加到“复习列表”中，形成简易的闪卡（Flashcard）。该列表支持导出为常见的格式（如CSV或Anki格式）。



## 二、 核心架构决策与深度解析

### 1. 客户端：Tauri V2 + Rust + SQLite (`sqlx`)

客户端是用户体验的直接载体，我们的选择旨在实现**原生性能、极致轻量与绝对安全**。

*   **框架: Tauri V2 (稳定版)**
    *   **理论依据:** Tauri V2 现已进入稳定迭代阶段（例如 v2.8+），其核心 API 成熟，适合生产环境。它通过 Rust 核心与系统 WebView 的结合，提供了接近原生应用的性能。
    *   **量化优势:**
        *   **资源占用:** 相比 Electron，Tauri 应用的内存占用通常能降低 **50%-70%**。
        *   **应用体积:** 最终打包体积可减小 **80%** 以上，实现真正的轻量化。
*   **本地数据库: SQLite 通过 `sqlx` 库**
    *   **理论依据:** SQLite 是一个零配置、ACID 兼容的嵌入式数据库，完美契合客户端应用场景。选择 `sqlx` 而非其他 ORM 是一个关键的工程决策。`sqlx` 是一个纯 Rust、异步的 SQL 工具包，其核心特性是**编译时 SQL 验证**。
    *   **量化优势:**
        *   **健壮性:** 在编译阶段即可捕获 SQL 语法错误、表名/字段名错误，预估可将 **30%-50%** 的潜在数据库运行时错误提前消除。
        *   **性能:** `sqlx` 避免了传统 ORM 的抽象开销，性能接近原生 SQL 查询。

### 2. 后端：FastAPI + Rust 性能核心 + Qdrant

后端是系统的处理中枢，采用混合编程模型来处理**性能瓶颈**。

*   **Web 框架: FastAPI (Python)**
    *   **理论依据:** FastAPI 的异步特性、基于 Pydantic 的数据验证和自动文档生成，使其成为构建高效、健壮 API 的行业标准。它作为“胶水层”，负责处理 Web 请求、业务逻辑流转。
*   **核心处理引擎: Rust 原生模块 (通过 PyO3)**
    *   **问题识别:** 文本解析（特别是大型 PDF/DOCX）是典型的 **CPU 密集型**任务，Python 的全局解释器锁 (GIL) 在此场景下会成为严重的性能瓶颈。
    *   **解决方案:** 我们**不自研解析器**，而是利用 Rust 生态中成熟的高性能库（如 `lopdf`, `docx-rs`, `extractous`），通过 `PyO3` 将其编译成 Python 可直接 `import` 的原生模块。
    *   **量化优势:**
        *   **性能提升:** 根据公开基准，使用 Rust 库进行文档解析，速度可比纯 Python 库快 **5x 到 25x**。
        *   **内存安全:** Rust 的所有权模型确保在处理大文件时不会出现内存泄漏或意外的内存峰值。
    *   **工程价值:** 这是整个项目**技术深度的核心展示点**。它证明了开发者有能力识别性能瓶颈，并采用最合适的工具（混合编程）进行精准优化。
*   **向量数据库: Qdrant**
    *   **理论依据:** Qdrant 是一个用 Rust 编写的高性能向量数据库。在 RAG（检索增强生成）应用中，检索速度和过滤能力直接决定了最终生成质量和响应延迟。
    *   **选择理由:**
        1.  **原生性能:** Rust 带来的低延迟和高吞吐量。在 P95 延迟（95% 的请求都能满足的延迟时间）指标上，通常优于其他方案。
        2.  **高级过滤:** 支持在向量搜索的同时进行复杂的元数据过滤，这对实现精准的上下文检索至关重要。
        3.  **异步客户端:** 提供与 FastAPI 完美契合的异步 Python 客户端，确保整个后端 I/O 链路非阻塞。
    *   **工程价值:** 表明开发者在选择基础设施时，会评估其底层实现和性能特征，而不仅仅是选择最流行或最简单的工具。

### 3. 跨平台策略：基于 Tauri V2 的移动端支持

*   **可行性:** Tauri V2 的稳定发布使其具备了可靠的 Android/iOS 支持能力。
*   **实现路径:**
    1.  **核心逻辑复用:** 客户端的 Rust 核心（API 调用、数据库交互、状态管理）是平台无关的，可直接复用。
    2.  **UI/UX 适配:** 前端 UI 需要进行响应式设计，以适应移动端屏幕尺寸和触摸交互。
    3.  **原生能力集成:** 通过 Tauri V2 的插件系统调用移动端特有 API（如文件系统权限、通知等）。
    4.  **构建流程:** CI/CD 管道需要扩展，增加针对 Android (SDK/NDK) 和 iOS (Xcode) 的构建目标。
*   **工程价值:** 展现了构建真正跨平台应用（桌面+移动）的视野和能力。

## 三、 常见问题 (FAQ)

**Q1: 为什么选择 Tauri V2 而不是更成熟的 Electron？**

**A:** 为了追求极致的性能和资源效率。Electron 通过捆绑完整的 Chromium 和 Node.js 运行时，导致应用体积庞大（通常 > 100MB）、内存占用高。Tauri 利用系统原生的 WebView，使得最终应用体积极小（通常 < 10MB），内存占用也显著降低。这体现了对最终用户体验和系统资源的尊重，是更高阶的工程考量。

**Q2: 在后端为“仅仅”一个解析功能引入 Rust，是否过于复杂（Overkill）？**

**A:** 并非如此。这是一种**外科手术式**的优化策略。我们没有用 Rust 重写整个后端，而是精准识别了整个系统中**最关键的性能瓶颈**——CPU 密集型的大文件处理，并只针对这一点使用最优工具。这种“80/20”法则的应用，即用 20% 的精力解决 80% 的性能问题，是成熟软件架构设计的体现。它展示了混合编程的务实价值，而不是盲目追求单一技术栈。

**Q3: 为什么选择 Qdrant 而不是更简单、更轻量的 ChromaDB 或 Faiss？**

**A:** 选择 Qdrant 是出于对**生产级性能和可扩展性**的考量。虽然 ChromaDB 在本地开发和原型验证中非常便捷，但 Qdrant 在设计上更侧重于高并发、低延迟和复杂的过滤查询，这些都是将一个项目从“玩具”提升到“产品”级别的关键。选择 Qdrant 表明开发者在技术选型时，已经考虑到了项目未来的性能需求和 RAG 管道的复杂性。